{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mpg_linear_regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIdT9iu_Z4Rb"
      },
      "source": [
        "# Predict fuel efficiency using Linear Regression ML Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHp3M9ZmrIxj"
      },
      "source": [
        "In a *regression* problem, the aim is to predict the output of a continuous value, like a price or a probability. Contrast this with a *classification* problem, where the aim is to select a class from a list of classes (for example, where a picture contains an apple or an orange, recognizing which fruit is in the picture).\n",
        "\n",
        "This tutorial uses the classic [Auto MPG](https://archive.ics.uci.edu/ml/datasets/auto+mpg) dataset and demonstrates how to build models to predict the fuel efficiency of the late-1970s and early 1980s automobiles. To do this, you will provide the models with a description of many automobiles from that time period. This description includes attributes like cylinders, displacement, horsepower, and weight.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hLEAK-GQ1tK"
      },
      "source": [
        "##Install and import the required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moB4tpEHxKB3"
      },
      "source": [
        "# Use seaborn for pairplot.\n",
        "!pip install -q seaborn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rRo8oNqZ-Rj"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Make NumPy printouts easier to read.\n",
        "np.set_printoptions(precision=3, suppress=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xQKvCJ85kCQ"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_72b0LCNbjx"
      },
      "source": [
        "## The Auto MPG dataset\n",
        "\n",
        "The dataset is available from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFh9ne3FZ-On"
      },
      "source": [
        "### Get the data\n",
        "First download and import the dataset using pandas. Also, save the file locally. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiX2FI4gZtTt"
      },
      "source": [
        "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
        "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
        "                'Acceleration', 'Model Year', 'Origin']\n",
        "\n",
        "raw_dataset = pd.read_csv(url, names=column_names,\n",
        "                          na_values='?', comment='\\t',\n",
        "                          sep=' ', skipinitialspace=True)\n",
        "\n",
        "# Save the dataset to a CSV file\n",
        "raw_dataset.to_csv('mpg.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnRnCE3wRiMf"
      },
      "source": [
        "Output some rows in the dataseet. MPG is the label (what we want to train the model to predict), the rest of the fields are the features. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oY3pMPagJrO"
      },
      "source": [
        "dataset = raw_dataset.copy()\n",
        "dataset[30:40]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MWuJTKEDM-f"
      },
      "source": [
        "### Inspecting and Cleaning the Data\n",
        "\n",
        "Use the Pandas Dataframe describe() function to explore the data. If you're wondering what transpose() does, run the cell with and without it. \n",
        "\n",
        "Notice, the count property for Horsepower is less than for the rest of the fields. Something must be amiss. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqDh2JafptjC"
      },
      "source": [
        "dataset.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMjowmMCS-xp"
      },
      "source": [
        "In the code below, isna() returns if a value is not a number. Sum() will return the number of values for each field that isn't a number. This explains why Horsepower above had a lower count than the rest of the fields. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEJHhN65a2VV"
      },
      "source": [
        "dataset.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UPN0KBHa_WI"
      },
      "source": [
        "The code below simply removes the rows that have invalid (non-numeric) data. In this case the 6 rows where the Horsepower field has a null value. An alternative would be to replace the null values with the mean Horsepower from the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZUDosChC1UN"
      },
      "source": [
        "dataset = dataset.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK1xFMKbTj0T"
      },
      "source": [
        "Use the Seaborn pairplot() function to review the joint distribution of the pairs of columns from the training set. You are trying to visualize patterns than indicate a cooorelation between different fields and the label (MPG)\n",
        "\n",
        "You can experiment with other fields or all the fields if you like. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ILdTNQIoeoy"
      },
      "source": [
        "\n",
        "import seaborn as sns\n",
        "sns.pairplot(dataset[['MPG', 'Horsepower', 'Acceleration', 'Model Year']], \n",
        "             diag_kind='kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XKitwaH4v8h"
      },
      "source": [
        "## One-Hot Encoding\n",
        "\n",
        "The `\"Origin\"` column is categorical, not numeric. So the next step is to one-hot encode the values in the column with [pd.get_dummies](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWNTD2QjBWFJ"
      },
      "source": [
        "dataset['Origin'] = dataset['Origin'].map({1: 'USA', \n",
        "                                           2: 'Europe', \n",
        "                                           3: 'Japan'})\n",
        "\n",
        "dataset = pd.get_dummies(dataset, \n",
        "                         columns=['Origin'], \n",
        "                         prefix='', prefix_sep='')\n",
        "\n",
        "\n",
        "dataset[30:40]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cuym4yvk76vU"
      },
      "source": [
        "## Split the data into training and test sets\n",
        "\n",
        "Now, split the dataset into a training set and a test set. You will use the test set in the final evaluation of your models.\n",
        "\n",
        "In this case, 80% of the data is randomly selected for training, and the rest will be used for testing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qn-IGhUE7_1H"
      },
      "source": [
        "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
        "test_dataset = dataset.drop(train_dataset.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLXefuWAWWLs"
      },
      "source": [
        "Let's describe the training data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi2FzC3T21jR"
      },
      "source": [
        "train_dataset.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS7ZryuYWb_j"
      },
      "source": [
        "Now, let's describe the test data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw7xO6-JWgce"
      },
      "source": [
        "test_dataset.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db7Auq1yXUvh"
      },
      "source": [
        "### Split features from labels\n",
        "\n",
        "Separate the target value—the \"label\"—from the features. This label is the value that you will train the model to predict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2sluJdCW7jN"
      },
      "source": [
        "train_features = train_dataset.copy()\n",
        "test_features = test_dataset.copy()\n",
        "\n",
        "train_labels = train_features.pop('MPG')\n",
        "test_labels = test_features.pop('MPG')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRklxK5s388r"
      },
      "source": [
        "## Normalization\n",
        "\n",
        "In the table of statistics it's easy to see how different the ranges of each feature are:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcmY6lKKbkw8"
      },
      "source": [
        "train_dataset.describe().transpose()[['mean', 'std']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ywmerQ6dSox"
      },
      "source": [
        "It is good practice to normalize features that use different scales and ranges.\n",
        "\n",
        "One reason this is important is because the features are multiplied by the model weights. So, the scale of the outputs and the scale of the gradients are affected by the scale of the inputs.\n",
        "\n",
        "Although a model *might* converge without feature normalization, normalization makes training much more stable.\n",
        "\n",
        "Note: There is no advantage to normalizing the one-hot features—it is done here for simplicity. For more details on how to use the preprocessing layers, refer to the [Working with preprocessing layers](https://www.tensorflow.org/guide/keras/preprocessing_layers) guide and the [Classify structured data using Keras preprocessing layers](../structured_data/preprocessing_layers.ipynb) tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFJ6ISropeoo"
      },
      "source": [
        "### The Normalization layer\n",
        "\n",
        "The `tf.keras.layers.Normalization` is a clean and simple way to add feature normalization into your model.\n",
        "\n",
        "The first step is to create the layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlC5ooJrgjQF"
      },
      "source": [
        "normalizer = tf.keras.layers.Normalization(axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYA2Ap6nVOha"
      },
      "source": [
        "Then, fit the state of the preprocessing layer to the data by calling `Normalization.adapt`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrBbbjbwV91f"
      },
      "source": [
        "normalizer.adapt(np.array(train_features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZccMR5yV9YV"
      },
      "source": [
        "Calculate the mean and variance, and store them in the layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGn-ukwxSPtx"
      },
      "source": [
        "print(normalizer.mean.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGWKaF9GSRuN"
      },
      "source": [
        "When the layer is called, it returns the input data, with each feature independently normalized:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l7zFL_XWIRu"
      },
      "source": [
        "first = np.array(train_features[:5])\n",
        "\n",
        "with np.printoptions(precision=2, suppress=True):\n",
        "  print('First example:', first)\n",
        "  print()\n",
        "  print('Normalized:', normalizer(first).numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFby9n0tnHkw"
      },
      "source": [
        "### Linear regression with one variable\n",
        "\n",
        "Begin with a single-variable linear regression to predict `'MPG'` from `'Horsepower'`.\n",
        "\n",
        "Training a model with `tf.keras` typically starts by defining the model architecture. Use a `tf.keras.Sequential` model, which [represents a sequence of steps](.././guide/keras/sequential_model.ipynb).\n",
        "\n",
        "There are two steps in your single-variable linear regression model:\n",
        "\n",
        "- Normalize the `'Horsepower'` input features using the `tf.keras.layers.Normalization` preprocessing layer.\n",
        "- Apply a linear transformation ($y = mx+b$) to produce 1 output using a linear layer (`tf.keras.layers.Dense`).\n",
        "\n",
        "The number of _inputs_ can either be set by the `input_shape` argument, or automatically when the model is run for the first time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp3gAFn3TPv8"
      },
      "source": [
        "First, create a NumPy array made of the `'Horsepower'` features. Then, instantiate the `tf.keras.layers.Normalization` and fit its state to the `horsepower` data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gJAy0fKs1TS"
      },
      "source": [
        "horsepower = np.array(train_features['Horsepower'])\n",
        "\n",
        "horsepower_normalizer = layers.Normalization(input_shape=[1,], axis=None)\n",
        "horsepower_normalizer.adapt(horsepower)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NVlHJY2TWlC"
      },
      "source": [
        "Build the Keras Sequential model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0sXM7qLlKfZ"
      },
      "source": [
        "horsepower_model = tf.keras.Sequential([\n",
        "    horsepower_normalizer,\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "horsepower_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSkanJlmmFBX"
      },
      "source": [
        "Once the model is built, configure the training procedure using the Keras `Model.compile` method. The most important arguments to compile are the `loss` and the `optimizer`, since these define what will be optimized (`mean_absolute_error`) and how (using the `tf.keras.optimizers.Adam`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxA_3lpOm-SK"
      },
      "source": [
        "horsepower_model.compile(\n",
        "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
        "    loss='mean_absolute_error')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3q1I9TwnRSC"
      },
      "source": [
        "Having configured the training configured, use Keras `Model.fit` to execute the training for 100 epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iSrNy59nRAp"
      },
      "source": [
        "%%time\n",
        "history = horsepower_model.fit(\n",
        "    train_features['Horsepower'],\n",
        "    train_labels,\n",
        "    epochs=100,\n",
        "    verbose=0,\n",
        "    # Calculate validation results on 20% of the training data.\n",
        "    validation_split = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQm3pc0FYPQB"
      },
      "source": [
        "Visualize the model's training progress using the stats stored in the `history` object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCAwD_y4AdC3"
      },
      "source": [
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "print(hist.head())\n",
        "print(hist.tail())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E54UoZunqhc"
      },
      "source": [
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "  plt.ylim([0, 10])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Error [MPG]')\n",
        "  plt.legend()\n",
        "  plt.grid(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYsQYrIZyqjz"
      },
      "source": [
        "plot_loss(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMNrt8X2ebXd"
      },
      "source": [
        "Collect the results on the test set for later:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDZ8EvNYrDtx"
      },
      "source": [
        "test_results = {}\n",
        "\n",
        "test_results['horsepower_model'] = horsepower_model.evaluate(\n",
        "    test_features['Horsepower'],\n",
        "    test_labels, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0qutYAKwoda"
      },
      "source": [
        "Since this is a single variable regression, it's easy to view the model's predictions as a function of the input:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDS2JEtOn9Jn"
      },
      "source": [
        "x = tf.linspace(0.0, 250, 251)\n",
        "y = horsepower_model.predict(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rttFCTU8czsI"
      },
      "source": [
        "def plot_horsepower(x, y):\n",
        "  plt.scatter(train_features['Horsepower'], train_labels, label='Data')\n",
        "  plt.plot(x, y, color='k', label='Predictions')\n",
        "  plt.xlabel('Horsepower')\n",
        "  plt.ylabel('MPG')\n",
        "  plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l9ZiAOEUNBL"
      },
      "source": [
        "plot_horsepower(x,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk2RmlqPoM9u"
      },
      "source": [
        "### Linear regression with multiple inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PribnwDHUksC"
      },
      "source": [
        "You can use an almost identical setup to make predictions based on multiple inputs. This model still does the same $y = mx+b$ except that $m$ is a matrix and $b$ is a vector.\n",
        "\n",
        "Create a two-step Keras Sequential model again with the first layer being `normalizer` (`tf.keras.layers.Normalization(axis=-1)`) you defined earlier and adapted to the whole dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssnVcKg7oMe6"
      },
      "source": [
        "all_features_normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "all_features_normalizer.adapt(np.array(train_features))\n",
        "\n",
        "\n",
        "linear_model = tf.keras.Sequential([\n",
        "    all_features_normalizer,\n",
        "    layers.Dense(units=1)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eINAc6rZXzOt"
      },
      "source": [
        "Configure the model with Keras `Model.compile` and train with `Model.fit` for 100 epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pARu7JVeZDv"
      },
      "source": [
        "linear_model.compile(\n",
        "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
        "    loss='mean_absolute_error')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZoOYORvoTSe"
      },
      "source": [
        "%%time\n",
        "history = linear_model.fit(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    epochs=100,\n",
        "    verbose=0,\n",
        "    # Calculate validation results on 20% of the training data.\n",
        "    validation_split = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdxiCbiNYK2F"
      },
      "source": [
        "Using all the inputs in this regression model achieves a much lower training and validation error than the `horsepower_model`, which had one input:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sWO3W0koYgu"
      },
      "source": [
        "plot_loss(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyN49hIWe_NH"
      },
      "source": [
        "Collect the results on the test set for later:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNC3D1DGsGgK"
      },
      "source": [
        "test_results['linear_model'] = linear_model.evaluate(\n",
        "    test_features, test_labels, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiCucdPLfMkZ"
      },
      "source": [
        "## Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDf1xebEfWBw"
      },
      "source": [
        "Since all models have been trained, you can review their test set performance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5_ooufM5iH2"
      },
      "source": [
        "pd.DataFrame(test_results, index=['Mean absolute error [MPG]']).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DABIVzsCf-QI"
      },
      "source": [
        "These results match the validation error observed during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft603OzXuEZC"
      },
      "source": [
        "### Make predictions\n",
        "\n",
        "You can now make predictions with the `dnn_model` on the test set using Keras `Model.predict` and review the loss:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe7RXH3N3CWU"
      },
      "source": [
        "test_predictions = linear_model.predict(test_features).flatten()\n",
        "\n",
        "a = plt.axes(aspect='equal')\n",
        "plt.scatter(test_labels, test_predictions)\n",
        "plt.xlabel('True Values [MPG]')\n",
        "plt.ylabel('Predictions [MPG]')\n",
        "lims = [0, 50]\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "_ = plt.plot(lims, lims)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19wyogbOSU5t"
      },
      "source": [
        "It appears that the model predicts reasonably well.\n",
        "\n",
        "Now, check the error distribution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-OHX4DiXd8x"
      },
      "source": [
        "error = test_predictions - test_labels\n",
        "plt.hist(error, bins=25)\n",
        "plt.xlabel('Prediction Error [MPG]')\n",
        "_ = plt.ylabel('Count')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSyaHUfDT-mZ"
      },
      "source": [
        "If you're happy with the model, save it for later use with `Model.save`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-WwLlmfT-mb"
      },
      "source": [
        "linear_model.save('my_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Benlnl8UT-me"
      },
      "source": [
        "If you reload the model, it gives identical output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyyyj2zVT-mf"
      },
      "source": [
        "reloaded = tf.keras.models.load_model('my_model')\n",
        "\n",
        "test_results['reloaded'] = reloaded.evaluate(\n",
        "    test_features, test_labels, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_GchJ2tg-2o"
      },
      "source": [
        "pd.DataFrame(test_results, index=['Mean absolute error [MPG]']).T"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}