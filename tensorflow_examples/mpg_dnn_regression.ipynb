{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mpg_dnn_regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIdT9iu_Z4Rb"
      },
      "source": [
        "# Predict fuel efficiency using DNN Regression ML Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHp3M9ZmrIxj"
      },
      "source": [
        "In a *regression* problem, the aim is to predict the output of a continuous value, like a price or a probability. Contrast this with a *classification* problem, where the aim is to select a class from a list of classes (for example, where a picture contains an apple or an orange, recognizing which fruit is in the picture).\n",
        "\n",
        "This tutorial uses the classic [Auto MPG](https://archive.ics.uci.edu/ml/datasets/auto+mpg) dataset and demonstrates how to build models to predict the fuel efficiency of the late-1970s and early 1980s automobiles. To do this, you will provide the models with a description of many automobiles from that time period. This description includes attributes like cylinders, displacement, horsepower, and weight.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hLEAK-GQ1tK"
      },
      "source": [
        "##Install and import the required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moB4tpEHxKB3"
      },
      "source": [
        "# Use seaborn for pairplot.\n",
        "!pip install -q seaborn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rRo8oNqZ-Rj"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Make NumPy printouts easier to read.\n",
        "np.set_printoptions(precision=3, suppress=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xQKvCJ85kCQ"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_72b0LCNbjx"
      },
      "source": [
        "## The Auto MPG dataset\n",
        "\n",
        "The dataset is available from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFh9ne3FZ-On"
      },
      "source": [
        "### Get the data\n",
        "First download and import the dataset using pandas. Also, save the file locally. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiX2FI4gZtTt"
      },
      "source": [
        "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
        "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
        "                'Acceleration', 'Model Year', 'Origin']\n",
        "\n",
        "raw_dataset = pd.read_csv(url, names=column_names,\n",
        "                          na_values='?', comment='\\t',\n",
        "                          sep=' ', skipinitialspace=True)\n",
        "\n",
        "# Save the dataset to a CSV file\n",
        "raw_dataset.to_csv('mpg.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnRnCE3wRiMf"
      },
      "source": [
        "Output some rows in the dataseet. MPG is the label (what we want to train the model to predict), the rest of the fields are the features. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oY3pMPagJrO"
      },
      "source": [
        "dataset = raw_dataset.copy()\n",
        "dataset[30:40]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MWuJTKEDM-f"
      },
      "source": [
        "### Inspecting and Cleaning the Data\n",
        "\n",
        "Use the Pandas Dataframe describe() function to explore the data. If you're wondering what transpose() does, run the cell with and without it. \n",
        "\n",
        "Notice, the count property for Horsepower is less than for the rest of the fields. Something must be amiss. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqDh2JafptjC"
      },
      "source": [
        "dataset.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMjowmMCS-xp"
      },
      "source": [
        "In the code below, isna() returns if a value is not a number. Sum() will return the number of values for each field that isn't a number. This explains why Horsepower above had a lower count than the rest of the fields. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEJHhN65a2VV"
      },
      "source": [
        "dataset.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UPN0KBHa_WI"
      },
      "source": [
        "The code below simply removes the rows that have invalid (non-numeric) data. In this case the 6 rows where the Horsepower field has a null value. An alternative would be to replace the null values with the mean Horsepower from the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZUDosChC1UN"
      },
      "source": [
        "dataset = dataset.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK1xFMKbTj0T"
      },
      "source": [
        "Use the Seaborn pairplot() function to review the joint distribution of the pairs of columns from the training set. You are trying to visualize patterns than indicate a cooorelation between different fields and the label (MPG)\n",
        "\n",
        "You can experiment with other fields or all the fields if you like. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ILdTNQIoeoy"
      },
      "source": [
        "\n",
        "import seaborn as sns\n",
        "sns.pairplot(dataset[['MPG', 'Horsepower', 'Acceleration', 'Model Year']], \n",
        "             diag_kind='kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XKitwaH4v8h"
      },
      "source": [
        "## One-Hot Encoding\n",
        "\n",
        "The `\"Origin\"` column is categorical, not numeric. So the next step is to one-hot encode the values in the column with [pd.get_dummies](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWNTD2QjBWFJ"
      },
      "source": [
        "dataset['Origin'] = dataset['Origin'].map({1: 'USA', \n",
        "                                           2: 'Europe', \n",
        "                                           3: 'Japan'})\n",
        "\n",
        "dataset = pd.get_dummies(dataset, \n",
        "                         columns=['Origin'], \n",
        "                         prefix='', prefix_sep='')\n",
        "\n",
        "\n",
        "dataset[30:40]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cuym4yvk76vU"
      },
      "source": [
        "## Split the data into training and test sets\n",
        "\n",
        "Now, split the dataset into a training set and a test set. You will use the test set in the final evaluation of your models.\n",
        "\n",
        "In this case, 80% of the data is randomly selected for training, and the rest will be used for testing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qn-IGhUE7_1H"
      },
      "source": [
        "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
        "test_dataset = dataset.drop(train_dataset.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLXefuWAWWLs"
      },
      "source": [
        "Let's describe the training data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi2FzC3T21jR"
      },
      "source": [
        "train_dataset.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS7ZryuYWb_j"
      },
      "source": [
        "Now, let's describe the test data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw7xO6-JWgce"
      },
      "source": [
        "test_dataset.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db7Auq1yXUvh"
      },
      "source": [
        "### Split features from labels\n",
        "\n",
        "Separate the target value—the \"label\"—from the features. This label is the value that you will train the model to predict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2sluJdCW7jN"
      },
      "source": [
        "train_features = train_dataset.copy()\n",
        "test_features = test_dataset.copy()\n",
        "\n",
        "train_labels = train_features.pop('MPG')\n",
        "test_labels = test_features.pop('MPG')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRklxK5s388r"
      },
      "source": [
        "## Normalization\n",
        "\n",
        "In the table of statistics it's easy to see how different the ranges of each feature are:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcmY6lKKbkw8"
      },
      "source": [
        "train_dataset.describe().transpose()[['mean', 'std']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ywmerQ6dSox"
      },
      "source": [
        "It is good practice to normalize features that use different scales and ranges.\n",
        "\n",
        "One reason this is important is because the features are multiplied by the model weights. So, the scale of the outputs and the scale of the gradients are affected by the scale of the inputs.\n",
        "\n",
        "Although a model *might* converge without feature normalization, normalization makes training much more stable.\n",
        "\n",
        "Note: There is no advantage to normalizing the one-hot features—it is done here for simplicity. For more details on how to use the preprocessing layers, refer to the [Working with preprocessing layers](https://www.tensorflow.org/guide/keras/preprocessing_layers) guide and the [Classify structured data using Keras preprocessing layers](../structured_data/preprocessing_layers.ipynb) tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFJ6ISropeoo"
      },
      "source": [
        "### The Normalization layer\n",
        "\n",
        "The `tf.keras.layers.Normalization` is a clean and simple way to add feature normalization into your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlC5ooJrgjQF"
      },
      "source": [
        "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "normalizer.adapt(np.array(train_features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZccMR5yV9YV"
      },
      "source": [
        "Calculate the mean and variance, and store them in the layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGn-ukwxSPtx"
      },
      "source": [
        "print(normalizer.mean.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGWKaF9GSRuN"
      },
      "source": [
        "When the layer is called, it returns the input data, with each feature independently normalized:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l7zFL_XWIRu"
      },
      "source": [
        "first = np.array(train_features[:5])\n",
        "\n",
        "with np.printoptions(precision=2, suppress=True):\n",
        "  print('First example:', first)\n",
        "  print()\n",
        "  print('Normalized:', normalizer(first).numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmjdzxKzEu1-"
      },
      "source": [
        "## Regression with a deep neural network (DNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT_aHPsrzO1t"
      },
      "source": [
        "Here, you will implement single-input and multiple-input DNN models.\n",
        "\n",
        "The code is basically the same as a linear model, except the model is expanded to include some \"hidden\" non-linear layers. The name \"hidden\" here just means not directly connected to the inputs or outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SWtkIjhrZwa"
      },
      "source": [
        "These models will contain a few more layers than a linear model:\n",
        "\n",
        "* The normalization layer.\n",
        "* Two hidden, non-linear, `Dense` layers with the ReLU (`relu`) activation function nonlinearity.\n",
        "* A linear `Dense` single-output layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c51caebbc0d"
      },
      "source": [
        "### Regression using a DNN and a single input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvu9gtxTZR5V"
      },
      "source": [
        "Create a DNN model with only `'Horsepower'` as input and `horsepower_normalizer` as the normalization layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGbPb-PHGbhs"
      },
      "source": [
        "horsepower = np.array(train_features['Horsepower'])\n",
        "\n",
        "horsepower_normalizer = layers.Normalization(input_shape=[1,], axis=None)\n",
        "horsepower_normalizer.adapt(horsepower)\n",
        "\n",
        "dnn_horsepower_model = keras.Sequential([\n",
        "      horsepower_normalizer,\n",
        "      layers.Dense(64, activation='relu'),\n",
        "      layers.Dense(64, activation='relu'),\n",
        "      layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "dnn_horsepower_model.compile(loss='mean_absolute_error',\n",
        "                             optimizer=tf.keras.optimizers.Adam(0.001))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj49Og4YGULr"
      },
      "source": [
        "This model has quite a few more trainable parameters than the linear models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReAD0n6MsFK-"
      },
      "source": [
        "dnn_horsepower_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-qWCsh6DlyH"
      },
      "source": [
        "Train the model with Keras `Model.fit`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD7qHCmNIOY0"
      },
      "source": [
        "%%time\n",
        "history = dnn_horsepower_model.fit(\n",
        "    train_features['Horsepower'],\n",
        "    train_labels,\n",
        "    validation_split=0.2,\n",
        "    verbose=0, epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGwCBuJwjp9K"
      },
      "source": [
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "print(hist.head())\n",
        "print(hist.tail())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f104Mcy3jf-h"
      },
      "source": [
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "  plt.ylim([0, 10])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Error [MPG]')\n",
        "  plt.legend()\n",
        "  plt.grid(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcF6UWjdCU8T"
      },
      "source": [
        "plot_loss(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG1snlpR2QCK"
      },
      "source": [
        "If you plot the predictions as a function of `'Horsepower'`, you should notice how this model takes advantage of the nonlinearity provided by the hidden layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPF53Rem14NS"
      },
      "source": [
        "x = tf.linspace(0.0, 250, 251)\n",
        "y = dnn_horsepower_model.predict(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0XvgMfMkFI5"
      },
      "source": [
        "def plot_horsepower(x, y):\n",
        "  plt.scatter(train_features['Horsepower'], train_labels, label='Data')\n",
        "  plt.plot(x, y, color='k', label='Predictions')\n",
        "  plt.xlabel('Horsepower')\n",
        "  plt.ylabel('MPG')\n",
        "  plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsf9rD8I17Wq"
      },
      "source": [
        "plot_horsepower(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxCJKIUpe4io"
      },
      "source": [
        "Collect the results on the test set for later:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJjM0dU52XtN"
      },
      "source": [
        "test_results = {}\n",
        "\n",
        "test_results['dnn_horsepower_model'] = dnn_horsepower_model.evaluate(\n",
        "    test_features['Horsepower'], test_labels,\n",
        "    verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_2Btebp2e64"
      },
      "source": [
        "### Regression using a DNN and multiple inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKFtezDldLSf"
      },
      "source": [
        "Repeat the previous process using all the inputs. The model's performance slightly improves on the validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0mhscXh2k36"
      },
      "source": [
        "all_features_normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "all_features_normalizer.adapt(np.array(train_features))\n",
        "\n",
        "dnn_model = keras.Sequential([\n",
        "      all_features_normalizer,\n",
        "      layers.Dense(64, activation='relu'),\n",
        "      layers.Dense(64, activation='relu'),\n",
        "      layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "dnn_model.compile(loss='mean_absolute_error',\n",
        "                             optimizer=tf.keras.optimizers.Adam(0.001))\n",
        "\n",
        "\n",
        "dnn_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXDENACl2tuW"
      },
      "source": [
        "%%time\n",
        "history = dnn_model.fit(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    validation_split=0.2,\n",
        "    verbose=0, epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9Dbj0fX23RQ"
      },
      "source": [
        "plot_loss(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWoVYS34fJPZ"
      },
      "source": [
        "Collect the results on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bZIa96W3c7K"
      },
      "source": [
        "test_results['dnn_model'] = dnn_model.evaluate(test_features, test_labels, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiCucdPLfMkZ"
      },
      "source": [
        "## Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDf1xebEfWBw"
      },
      "source": [
        "Since all models have been trained, you can review their test set performance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5_ooufM5iH2"
      },
      "source": [
        "pd.DataFrame(test_results, index=['Mean absolute error [MPG]']).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DABIVzsCf-QI"
      },
      "source": [
        "These results match the validation error observed during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft603OzXuEZC"
      },
      "source": [
        "### Make predictions\n",
        "\n",
        "You can now make predictions with the `dnn_model` on the test set using Keras `Model.predict` and review the loss:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe7RXH3N3CWU"
      },
      "source": [
        "test_predictions = dnn_model.predict(test_features).flatten()\n",
        "\n",
        "a = plt.axes(aspect='equal')\n",
        "plt.scatter(test_labels, test_predictions)\n",
        "plt.xlabel('True Values [MPG]')\n",
        "plt.ylabel('Predictions [MPG]')\n",
        "lims = [0, 50]\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "_ = plt.plot(lims, lims)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19wyogbOSU5t"
      },
      "source": [
        "It appears that the model predicts reasonably well.\n",
        "\n",
        "Now, check the error distribution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-OHX4DiXd8x"
      },
      "source": [
        "error = test_predictions - test_labels\n",
        "plt.hist(error, bins=25)\n",
        "plt.xlabel('Prediction Error [MPG]')\n",
        "_ = plt.ylabel('Count')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSyaHUfDT-mZ"
      },
      "source": [
        "If you're happy with the model, save it for later use with `Model.save`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-WwLlmfT-mb"
      },
      "source": [
        "dnn_model.save('my_dnn_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Benlnl8UT-me"
      },
      "source": [
        "If you reload the model, it gives identical output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyyyj2zVT-mf"
      },
      "source": [
        "reloaded = tf.keras.models.load_model('my_dnn_model')\n",
        "\n",
        "test_results['reloaded'] = reloaded.evaluate(\n",
        "    test_features, test_labels, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_GchJ2tg-2o"
      },
      "source": [
        "pd.DataFrame(test_results, index=['Mean absolute error [MPG]']).T"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}